{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hernia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Paru\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img.tolist()\n",
    "\n",
    "image_path = './img_test.png'\n",
    "image = prepare_image(image_path)\n",
    "\n",
    "data = {\"instances\": image}  # Konversi ke daftar Python agar dapat di-serialize menjadi JSON\n",
    "\n",
    "url = 'https://medical-math-model-paru.onrender.com/v1/models/paru:predict'\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "predict = response.json()\n",
    "class_labels = np.array(['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema',\n",
    "       'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration',\n",
    "       'Mass', 'Nodule', 'Normal', 'Pleural_Thickening', 'Pneumonia',\n",
    "       'Pneumothorax'])\n",
    "predicted_label = class_labels[np.argmax(predict['predictions'])]\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "URL_MODEL_JANTUNG = \"https://medical-math-model-jantung.onrender.com/v1/models\"\n",
    "\n",
    "with open('./tokenizer.json', 'r') as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"x\", filters='!\"#$%&*.,:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.word_index = word_index\n",
    "\n",
    "mlbs_loaded = {}\n",
    "\n",
    "for key in ['DU', 'DS', 'OB']:\n",
    "    with open(f'labels/classes_{key}.json', 'r') as f:\n",
    "        classes = json.load(f)\n",
    "    \n",
    "    # Create a new MultiLabelBinarizer and fit it with the loaded classes\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit([classes])\n",
    "    mlbs_loaded[key] = mlb\n",
    "\n",
    "# @title Function Declaration\n",
    "def akt(angka):\n",
    "    angka_dict = {\n",
    "        0: 'nol', 1: 'satu', 2: 'dua', 3: 'tiga', 4: 'empat', 5: 'lima',\n",
    "        6: 'enam', 7: 'tujuh', 8: 'delapan', 9: 'sembilan', 10: 'sepuluh',\n",
    "        11: 'sebelas', 12: 'dua belas', 13: 'tiga belas', 14: 'empat belas',\n",
    "        15: 'lima belas', 16: 'enam belas', 17: 'tujuh belas', 18: 'delapan belas',\n",
    "        19: 'sembilan belas', 20: 'dua puluh', 30: 'tiga puluh', 40: 'empat puluh',\n",
    "        50: 'lima puluh', 60: 'enam puluh', 70: 'tujuh puluh', 80: 'delapan puluh',\n",
    "        90: 'sembilan puluh', 100: 'seratus', 200: 'dua ratus', 300: 'tiga ratus',\n",
    "        400: 'empat ratus', 500: 'lima ratus', 600: 'enam ratus', 700: 'tujuh ratus',\n",
    "        800: 'delapan ratus', 900: 'sembilan ratus', 1000: 'seribu'\n",
    "    }\n",
    "\n",
    "    if isinstance(angka, float):\n",
    "        integer_part, decimal_part = str(angka).split('.')\n",
    "        integer_part = int(integer_part)\n",
    "        decimal_part = int(decimal_part)\n",
    "\n",
    "        return akt(integer_part) + ' koma ' + ' '.join(angka_dict[int(digit)] for digit in str(decimal_part))\n",
    "\n",
    "    if '/' in str(angka):\n",
    "        pembilang, penyebut = str(angka).split('/')\n",
    "        return akt(int(pembilang)) + ' per ' + akt(int(penyebut))\n",
    "\n",
    "    angka = int(angka)  # memastikan angka adalah integer jika bukan float atau fraksi\n",
    "\n",
    "    if angka in angka_dict:\n",
    "        return angka_dict[angka]\n",
    "\n",
    "    if angka < 100:\n",
    "        puluhan, sisa = divmod(angka, 10)\n",
    "        return angka_dict[puluhan * 10] + ' ' + angka_dict[sisa]\n",
    "\n",
    "    if angka < 1000:\n",
    "        ratusan, sisa = divmod(angka, 100)\n",
    "        if sisa == 0:\n",
    "            return angka_dict[ratusan * 100]\n",
    "        elif sisa < 10:\n",
    "            return angka_dict[ratusan * 100] + ' dan ' + angka_dict[sisa]\n",
    "        elif sisa < 100:\n",
    "            return angka_dict[ratusan * 100] + ' ' + akt(sisa)\n",
    "\n",
    "    raise ValueError(\"Angka di luar jangkauan fungsi ini\")\n",
    "\n",
    "def convert_prefix(content):\n",
    "    return ' '.join('positif' if char == '+' else 'negatif' for char in content)\n",
    "\n",
    "def convert_data(input_str):\n",
    "    prefix_map = {\n",
    "        'ronkhi': 'ronkhi', 'ves': 'ves', 'wheezing': 'wheezing',\n",
    "        'dingin': 'dingin', 'edema': 'edema', 'hangat': 'hangat'\n",
    "    }\n",
    "    prefix = next((p for p in prefix_map if input_str.startswith(p)), None)\n",
    "    if not prefix:\n",
    "        return \"Unknown prefix\"\n",
    "\n",
    "    content = input_str.replace(f'{prefix} ', '').strip('()')\n",
    "    parts = content.split('/')\n",
    "    description = f\"{prefix} {convert_prefix(parts[0])} per {convert_prefix(parts[1])}\"\n",
    "    return description\n",
    "\n",
    "def standardize_text(text, replacements):\n",
    "    text = text.lower().strip()\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def for_pulmo(text):\n",
    "    replacements = {\n",
    "        'rhonki': 'ronkhi', ' -': ' ', 'i(': 'i (', 'g(': 'g (',\n",
    "        'ves normal': '-', 'ves/ves': '-', '(+)': '(+/+)'\n",
    "    }\n",
    "    return standardize_text(text, replacements)\n",
    "\n",
    "def for_cor(text):\n",
    "    replacements = {\n",
    "        'ireguler': 'irregular', 'irreguler': 'irregular',\n",
    "        's1 dan s2 irregular': 's1 s2 single irregular',\n",
    "        's1 dan s2 tunggal irregular': 's1 s2 single irregular',\n",
    "        's1 dan s2 tunggal reguler': 's1 s2 single reguler',\n",
    "        's1 dan s2 reguler': 's1 s2 single reguler',\n",
    "        's1 dan s2 tunggal': 's1 s2 single',\n",
    "    }\n",
    "    return standardize_text(text, replacements)\n",
    "\n",
    "def for_abdomen(text):\n",
    "    replacements = {'nomal': 'normal'}\n",
    "    return standardize_text(text, replacements)\n",
    "\n",
    "def for_ext(text):\n",
    "    replacements = {'odem': 'edema', 'hagat': 'hangat', '(++)': '(+/+)'}\n",
    "    return standardize_text(text, replacements)\n",
    "\n",
    "def for_tambahan(text):\n",
    "    replacements = {}\n",
    "    return standardize_text(text, replacements)\n",
    "\n",
    "def paragraph_id(data):\n",
    "    gender = \"Laki-laki\" if data[\"gender\"] == \"L\" else \"Perempuan\"\n",
    "\n",
    "    def get_value(key, unit):\n",
    "        try:\n",
    "            return f\"{akt(float(data[key]))} {unit}\"\n",
    "        except (KeyError, ValueError):\n",
    "            return \"-\"\n",
    "\n",
    "    keluhan = (\n",
    "        f\"Pasien memiliki keluhan: {', '.join(data['keluhan']) if isinstance(data['keluhan'], list) else data['keluhan']}.\"\n",
    "        if data['keluhan'] != \"Tidak ada keluhan\"\n",
    "        else \"Pasien tidak memiliki keluhan.\"\n",
    "    )\n",
    "\n",
    "    kalium = get_value(\"kalium\", \"mmol/L\")\n",
    "    natrium = get_value(\"natrium\", \"mmol/L\")\n",
    "    kreatinin = get_value(\"kreatinin\", \"mg/dL\")\n",
    "\n",
    "    def pemeriksaan(key, func):\n",
    "        return f\"Pemeriksaan {key}: {', '.join([func(d) for d in data[key]])}.\" if key in data else \"\"\n",
    "\n",
    "    HR = f\"{akt(int(data['hr']))} bpm\" if data[\"hr\"] != \"-\" else \"-\"\n",
    "    TD = f\"{akt(data['td'])} mmHg\" if data[\"td\"] != \"-\" else \"-\"\n",
    "    LVEF = f\"{akt(float(data['lvef']))} persen\" if data['lvef'] != \"-\" else \"-\"\n",
    "    BB = f\"{akt(float(data['bb']))}\" if data['bb'] != \"-\" else \"-\"\n",
    "    TB = f\"{akt(int(data['tb']))}\" if data['tb'] != \"-\" else \"-\"\n",
    "\n",
    "    paragraf = [\n",
    "        f\"Seorang pasien {gender} berusia {akt(int(data['usia']))} tahun dengan berat badan {BB} kg dan tinggi badan {TB} cm.\",\n",
    "        keluhan,\n",
    "        f\"Hasil pemeriksaan menunjukkan LVEF sebesar {LVEF}.\",\n",
    "        pemeriksaan(\"cor\", for_cor),\n",
    "        pemeriksaan(\"pulmo\", lambda d: (for_pulmo(d))),\n",
    "        pemeriksaan(\"abdomen\", for_abdomen),\n",
    "        pemeriksaan(\"ext\", lambda d: (for_ext(d))),\n",
    "        pemeriksaan(\"tambahan\", for_tambahan) if data.get(\"tambahan\", [\"-\"])[0] != \"-\" else \"\",\n",
    "        f\"Tekanan darah pasien adalah {TD} dengan denyut jantung {HR}.\",\n",
    "        f\"Kadar Kalium {kalium}, Natrium {natrium}, dan Kreatinin {kreatinin}.\",\n",
    "    ]\n",
    "\n",
    "    return \" \".join(filter(None, paragraf)).lower()\n",
    "\n",
    "def url_and_threshold(key):\n",
    "    return {\n",
    "        \"DU\": (\n",
    "            URL_MODEL_JANTUNG + \"/du:predict\",\n",
    "            0.2,\n",
    "        ),\n",
    "        \"DS\": (\n",
    "            URL_MODEL_JANTUNG + \"/ds:predict\",\n",
    "            0.1,\n",
    "        ),\n",
    "        \"OB\": (\n",
    "            URL_MODEL_JANTUNG + \"/ob:predict\",\n",
    "            0.2,\n",
    "        ),\n",
    "    }.get(key)\n",
    "\n",
    "def predict_text(texts, key, tokenizer, max_len=128):\n",
    "    # Tokenisasi dan padding teks\n",
    "    sequences = tokenizer.texts_to_sequences([texts])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    url, threshold = url_and_threshold(key)\n",
    "    response = requests.post(url, json={\"instances\": padded_sequences.tolist()})\n",
    "    response.raise_for_status()\n",
    "\n",
    "    probabilities = response.json()[\"predictions\"][0]\n",
    "    return ((np.array(probabilities) > threshold).astype(int)).tolist(), probabilities\n",
    "\n",
    "def transform_label(mlbs, key, label):\n",
    "    return list(mlbs[key].inverse_transform(np.array([label]))[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"nama\": \"rafli\",\n",
    "    \"domisili\": \"loloan\",\n",
    "    \"gender\": \"L\",\n",
    "    \"usia\": \"23\",\n",
    "    \"bb\": \"100\",\n",
    "    \"tb\": \"172\",\n",
    "    \"lvef\": \"23\",\n",
    "    \"keluhan\": \"tidak ada keluhan\",\n",
    "    \"td\": \"120/90\",\n",
    "    \"hr\": \"23\",\n",
    "    \"kalium\": \"23\",\n",
    "    \"natrium\": \"12\",\n",
    "    \"kreatinin\": \"123\",\n",
    "    \"cor\": [\n",
    "        \"123\",\n",
    "        \"awd\"\n",
    "    ],\n",
    "    \"pulmo\": [\n",
    "        \"123\",\n",
    "        \"123\"\n",
    "    ],\n",
    "    \"abdomen\": [\n",
    "        \"123\"\n",
    "    ],\n",
    "    \"ext\": [\n",
    "        \"123\"\n",
    "    ],\n",
    "    \"tambahan\": [\n",
    "        \"123\"\n",
    "    ]\n",
    "}\n",
    "data_paragraph = paragraph_id(data)\n",
    "data_paragraph\n",
    "\n",
    "keys = [\"DU\", \"DS\", \"OB\"]\n",
    "\n",
    "data_resp = {}\n",
    "for key in keys:\n",
    "    pred, prob = predict_text(data_paragraph, key, tokenizer)\n",
    "    label = transform_label(mlbs_loaded, key, pred)\n",
    "    data_resp[key] = {\"prob\":prob, \"pred\": pred, \"label\": label}\n",
    "data_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
